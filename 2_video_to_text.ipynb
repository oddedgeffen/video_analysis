{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a613e97e",
   "metadata": {},
   "source": [
    "# Video Speech-to-Text Extraction\n",
    "\n",
    "This notebook extracts speech from video files and converts it to text using the faster-whisper model (an optimized version of OpenAI's Whisper).\n",
    "\n",
    "Features:\n",
    "- Extracts speech segments with timestamps\n",
    "- Generates full transcript\n",
    "- Saves results to JSON\n",
    "- Works on CPU\n",
    "- Automatic dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29853f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from moviepy.editor import VideoFileClip\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5150a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(video_path: str, audio_path: str = \"temp_audio.wav\") -> str:\n",
    "    \"\"\"\n",
    "    Extract audio from video file and save it temporarily\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        audio_path: Path to save temporary audio file\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved audio file\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
    "    video.close()\n",
    "    return audio_path\n",
    "\n",
    "def transcribe_audio(\n",
    "    audio_path: str,\n",
    "    video_path: str,\n",
    "    model_size: str = \"tiny.en\",\n",
    "    language: str = None,\n",
    "    use_vad: bool = True  # Enable VAD by default now that we have onnxruntime\n",
    ") -> Tuple[List[Dict[str, Union[float, str]]], str]:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using faster-whisper\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        model_size: Size of the Whisper model to use\n",
    "        language: Language code (e.g., 'en' for English) or None for auto-detection\n",
    "        use_vad: Whether to use Voice Activity Detection (requires onnxruntime)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of segments, full transcript)\n",
    "    \"\"\"\n",
    "    # Check if CUDA (GPU) is available\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    compute_type = \"float16\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Load model\n",
    "    # Configure environment for OpenMP\n",
    "    \n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "\n",
    "    # Initialize model with stable settings for GTX 1050\n",
    "    model = WhisperModel(\n",
    "        \"base\",\n",
    "        device=\"cuda\",          # Use GPU\n",
    "        compute_type=\"float32\", # Most stable for GTX 1050\n",
    "        cpu_threads=4,         # Limit CPU threads\n",
    "        num_workers=1          # Reduce worker threads\n",
    "    )\n",
    "\n",
    "      \n",
    "    # Transcribe with GPU monitoring\n",
    "    print(\"\\nStarting transcription...\")\n",
    "    \n",
    "    # Transcribe with conservative settings\n",
    "    segments, info = model.transcribe(\n",
    "        \"temp_audio.wav\",\n",
    "        language=\"en\",\n",
    "        beam_size=10,           # Conservative beam size\n",
    "        vad_filter=False,       # Use VAD to skip silence\n",
    "        initial_prompt=None,   # No prompt needed\n",
    "        word_timestamps=True  # Disable word timestamps to save memory\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Process segments\n",
    "    results = []\n",
    "    full_text = []\n",
    "    \n",
    "    # Get video metadata first (we'll need fps for frame calculations)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Calculate frame numbers for this segment\n",
    "        start_frame = int(segment.start * fps)\n",
    "        end_frame = int(segment.end * fps)\n",
    "        \n",
    "        segment_dict = {\n",
    "            \"start\": segment.start,\n",
    "            \"end\": segment.end,\n",
    "            \"text\": segment.text.strip(),\n",
    "            \"frames\": {\n",
    "                \"start_frame\": start_frame,\n",
    "                \"end_frame\": end_frame,\n",
    "                \"frame_count\": end_frame - start_frame\n",
    "            }\n",
    "        }\n",
    "        results.append(segment_dict)\n",
    "        full_text.append(segment.text.strip())\n",
    "    \n",
    "    return results, \" \".join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed2d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_text(\n",
    "    video_path: str,\n",
    "    output_json: str = None,\n",
    "    model_size: str = \"base\",\n",
    "    language: str = None,\n",
    "    cleanup: bool = True,\n",
    "    use_vad: bool = False  # Enable VAD by default\n",
    ") -> Dict[str, Union[List[Dict[str, Union[float, str]]], str]]:\n",
    "    \"\"\"\n",
    "    Extract speech from video and convert to text\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_json: Path to save JSON output (optional)\n",
    "        model_size: Size of the Whisper model to use\n",
    "        language: Language code or None for auto-detection\n",
    "        cleanup: Whether to remove temporary audio file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing segments, full transcript, and video metadata\n",
    "    \"\"\"\n",
    "    # Get video metadata\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "    cap.release()\n",
    "    \n",
    "    # Extract audio\n",
    "    temp_audio = \"temp_audio.wav\"\n",
    "    audio_path = extract_audio(video_path, temp_audio)\n",
    "    print('audio_path', audio_path)\n",
    "    # Transcribe\n",
    "    segments, full_text = transcribe_audio(audio_path, video_path, model_size, language, use_vad)\n",
    "    \n",
    "    # Create result dictionary with video metadata\n",
    "    result = {\n",
    "        \"video_metadata\": {\n",
    "            \"fps\": fps,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"duration_seconds\": duration\n",
    "        },\n",
    "        \"segments\": segments,\n",
    "        \"full_text\": full_text\n",
    "    }\n",
    "    \n",
    "    # Save to JSON if requested\n",
    "    if output_json:\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Cleanup temporary audio file\n",
    "    if cleanup:\n",
    "        Path(audio_path).unlink(missing_ok=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470ca81",
   "metadata": {},
   "source": [
    "# Example Usage\n",
    "\n",
    "Let's try the transcription function with a sample video file. For this example, we'll save the output to `transcript.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path temp_audio.wav\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1050 with Max-Q Design\n",
      "\n",
      "Starting transcription...\n",
      "\n",
      "Transcription completed successfully!\n",
      "\n",
      "Full transcript:\n",
      "--------------------------------------------------------------------------------\n",
      "Here's a simple trick for being more confident when approaching women. Mindset matters. Most guys do this. They approach an attractive woman to see if she's interested. You allow her to be the chooser just because she's good looking. You're not going to do this. When you approach a woman next, you're going to have this mindset. Okay, she's attractive, but is she somebody I would want to be with? This changes your approach because you're not putting her on a pedestal. Instead, you're seeing if she's someone you would want to be with.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed segments have been saved to: transcript.json\n",
      "Number of segments: 10\n"
     ]
    }
   ],
   "source": [
    "# Example video file path (replace with your video file)\n",
    "video_path = \"recorded_video_yummy.mp4\"\n",
    "output_json = \"transcript.json\"\n",
    "\n",
    "# Run transcription\n",
    "try:\n",
    "    result = video_to_text(\n",
    "        video_path=video_path,\n",
    "        output_json=output_json,\n",
    "        model_size=\"base\",  # Options: tiny, base, small, medium, large\n",
    "        language='en',  # Auto-detect language\n",
    "        cleanup=True  # Remove temporary audio file\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTranscription completed successfully!\")\n",
    "    print(\"\\nFull transcript:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"full_text\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\nDetailed segments have been saved to: {output_json}\")\n",
    "    print(f\"Number of segments: {len(result['segments'])}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Video file '{video_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
